{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа № 1 \n",
    "## Выполнение разведочного анализа больших данных с использованием фреймворка Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 2\n",
    "\n",
    "В данной части работы рассмотрены:\n",
    "* разведочный анализ данных;\n",
    "* работа с Dataframe API фреймворка `Apache Spark`.\n",
    "\n",
    "Подключаем необходимые библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, sum, mean, when,\n",
    "    explode, count, desc, floor,\n",
    "    corr, array_contains, lit, first\n",
    ")\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем объект конфигурации для `Apache Spark`, указав необходимые параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_configuration() -> SparkConf:\n",
    "    \"\"\"\n",
    "    Создает и конфигурирует экземпляр SparkConf для приложения Spark.\n",
    "\n",
    "    Returns:\n",
    "        SparkConf: Настроенный экземпляр SparkConf.\n",
    "    \"\"\"\n",
    "    # Получаем имя пользователя\n",
    "    user_name = os.getenv(\"USER\")\n",
    "    \n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(\"lab 1 Test\")\n",
    "    conf.setMaster(\"yarn\")\n",
    "    conf.set(\"spark.submit.deployMode\", \"client\")\n",
    "    conf.set(\"spark.executor.memory\", \"12g\")\n",
    "    conf.set(\"spark.executor.cores\", \"8\")\n",
    "    conf.set(\"spark.executor.instances\", \"2\")\n",
    "    conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    conf.set(\"spark.driver.cores\", \"2\")\n",
    "    conf.set(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.0\")\n",
    "    conf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog.warehouse\", f\"hdfs:///user/{user_name}/warehouse\")\n",
    "    conf.set(\"spark.sql.catalog.spark_catalog.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\")\n",
    "\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём сам объект конфигурации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = create_spark_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём и выводим на экран сессию `Apache Spark`. В процессе создания сессии происходит подключение к кластеру `Apache Hadoop`, что может занять некоторое время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Укажем базу данных, которая была создана в первой части лабораторной работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"Efremenkov_database\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим созданную базу данных как текущую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем сохранённую в предыдущей части работы таблицу и загрузим её в `Spark Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"sobd_lab1_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем прочитанную таблицу на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на схему данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим количество строк в датафрейме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `_c0`\n",
    "Поскольку первичным ключём является столбец `LMK_KEY`, то номер строки нам не нужен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"_c0\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `LMK_KEY`\n",
    "\n",
    "Отсортируем датафрейм по столбцу `LMK_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy(\"LMK_KEY\", ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что некоторые значения ключа довольно странные (представляют собой больше описание, чем ключ). Удалим из датафрейма строки с такими ключами, оставив только подходящие по шаблону."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Регулярное выражение для VIN длиной 17 символов, состоящих из цифр и букв\n",
    "key_pattern = r\"^[a-f0-9]+$\"\n",
    "\n",
    "# Фильтрация DataFrame\n",
    "df = df.filter(col(\"LMK_KEY\").rlike(key_pattern))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим наличие дубликатов в датафрейме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .groupBy(\"LMK_KEY\")\n",
    "    .count()\n",
    "    .where(\"count > 1\")\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дубликатов нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `ADDRESS`\n",
    "\n",
    "Посмотрим внимательно на значения в столбце. Видно, что в данном столбце расположен **категориальный признак**.\n",
    "\n",
    "Введем функцию, определяющую количество NULL-значений в столбце."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nulls(data: DataFrame,\n",
    "                column_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Подсчет количества null и not null значений в указанном столбце.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame, содержащий данные.\n",
    "        column_name (str): Имя столбца для подсчета null и not null значений.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Подсчет количества null значений в указанном столбце\n",
    "    null_counts = data.select(\n",
    "        sum(col(column_name).isNull().cast(\"int\"))\n",
    "    ).collect()[0][0]\n",
    "\n",
    "    # Подсчет количества not null значений в указанном столбце\n",
    "    not_null_counts = data.select(\n",
    "        sum(col(column_name).isNotNull().cast(\"int\"))\n",
    "    ).collect()[0][0]\n",
    "\n",
    "    # Вывод результатов\n",
    "    print(f\"Число строк с NULL: {null_counts} \"\n",
    "          f\"({100 * null_counts / (null_counts + not_null_counts):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"ADDRESS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменим пропущенные значения на Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({\"ADDRESS\": \"Unknown\"})\n",
    "count_nulls(data=df, column_name=\"ADDRESS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `ADDRESS2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"ADDRESS2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, ситуация аналогичная. Удаляем столбец."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"ADDRESS2\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `ADDRESS3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"ADDRESS3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что более 90% данных в столбце пропущены. Можно, конечно, попытаться обработать то, что есть, но в целях упрощения анализа просто удалим столбец."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"ADDRESS3\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `CURRENT_ENERGY_EFFICIENCY` - колличественный признак\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"CURRENT_ENERGY_EFFICIENCY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пустых значений не обнаружено"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `PROPERTY_TYPE`\n",
    "\n",
    "Посмотрим внимательно на значения в столбце. Видно, что в данном столбце расположен **категориальный признак**.\n",
    "\n",
    "Введем функцию, определяющую количество NULL-значений в столбце."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"PROPERTY_TYPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что столбцы `PROPERTY_TYPE` не содержит пропущенных значений. Если бы признак их содержал, то логично заменить пропущенные значения на категорию `Unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({\"PROPERTY_TYPE\": \"Unknown\"})\n",
    "count_nulls(data=df, column_name=\"PROPERTY_TYPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию расчета и визуализации распределения категориальных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cat_distribution(data: DataFrame,\n",
    "                          column_name: str,\n",
    "                          top_n: int = 20) -> None:\n",
    "    \"\"\"\n",
    "    Построение распределения категориального признака.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame, содержащий данные.\n",
    "        column_name (str): Имя столбца для группировки.\n",
    "        top_n (int): Количество топ-значений для отображения.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Группировка данных по столбцу и подсчет количества\n",
    "    categories = (\n",
    "        data\n",
    "        .groupBy(column_name)\n",
    "        .count()\n",
    "        .orderBy(\"count\", ascending=False)\n",
    "    )\n",
    "    \n",
    "    print(f\"Количество категорий признака {column_name}: {categories.count()}\")\n",
    "\n",
    "    categories = (\n",
    "        categories\n",
    "        .limit(top_n)\n",
    "        .toPandas()\n",
    "    )\n",
    "    \n",
    "    # Визуализация с использованием Seaborn\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=column_name, y=\"count\", data=categories)\n",
    "    plt.title(f\"Barplot of \\\"{column_name}\\\" counts\")\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_distribution(data=df, column_name=\"PROPERTY_TYPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что недвижимость в представленном датасете имеются 5 определенных типов мест проживания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `INSPECTION_DATE`\n",
    "Признак **дата**. Уберем пустые значения при наличии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"INSPECTION_DATE\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `HEATING_COST_CURRENT`\n",
    "\n",
    "Данный столбец, согласно описанию и значениям, которые он принимает, можно отнести к **количественным**.\n",
    "\n",
    "Выполним аналогичные шаги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"HEATING_COST_CURRENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропущено много значений, но меньше половины. Стоит попытаться сохранить признак и обработать его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(data=df, columns=[\"HEATING_COST_CURRENT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно наблюдать сильные выбросы. Удалим строки, их содержащие, и убедимся, что потеряна небольшая часть данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(col(\"HEATING_COST_CURRENT\") > 8000).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменим пропуски средним значением признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(col(\"HEATING_COST_CURRENT\") < 8000)\n",
    "mean_cost = df.select(mean(col(\"HEATING_COST_CURRENT\"))).collect()[0][0]\n",
    "mean_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({\"HEATING_COST_CURRENT\": mean_cost})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вырежем также строки со значениями <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(col(\"HEATING_COST_CURRENT\") <= 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(col(\"HEATING_COST_CURRENT\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quant_distribution(data=df, column=\"HEATING_COST_CURRENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `HOT_WATER_COST_CURRENT`\n",
    "\n",
    "Аналогично выполняем действия со столбцом HOT_WATER_COST_CURRENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"HOT_WATER_COST_CURRENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(col(\"HOT_WATER_COST_CURRENT\") != 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `TOTAL_FLOOR_AREA`\n",
    "\n",
    "В соответствии с описанием и содержанием датасета логично считать данный признак **количественным**. Проверим его на наличие пропущенных значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"TOTAL_FLOOR_AREA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что пропуски в данном столбце отсутствуют.\n",
    "\n",
    "Создадим функцию, позволяющую рассчитывать статистические показатели данных в столбцах и строить диаграмму \"ящик с усами\" для оценки наличия выбросов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(data: DataFrame,\n",
    "                  columns: list[str],\n",
    "                  sample_fraction: float = 0.1) -> None:\n",
    "    \"\"\"\n",
    "    Построение boxplot для нескольких столбцов в PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame, содержащий данные.\n",
    "        columns (list of str): Список имен столбцов для построения boxplot.\n",
    "        sample_fraction (float): Доля данных для семплирования выбросов.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    box_data = []\n",
    "\n",
    "    for column in columns:\n",
    "        # Вычисление квантилей\n",
    "        quantiles = data.approxQuantile(column, [0.25, 0.5, 0.75], 0.01)\n",
    "        q1, median, q3 = quantiles\n",
    "\n",
    "        # Вычисление IQR и границ усов\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "        # Фильтрация выбросов\n",
    "        filtered_df = data.filter((col(column) >= lower_bound) & (col(column) <= upper_bound))\n",
    "        outliers_df = data.filter((col(column) < lower_bound) | (col(column) > upper_bound))\n",
    "\n",
    "        # Вычисление минимального и максимального значений\n",
    "        min_value = data.agg({column: \"min\"}).collect()[0][0]\n",
    "        mean_value = data.agg({column: \"mean\"}).collect()[0][0]\n",
    "        std_value = data.agg({column: \"std\"}).collect()[0][0]\n",
    "        max_value = data.agg({column: \"max\"}).collect()[0][0]\n",
    "\n",
    "        # Ограничение усов минимальным и максимальным значениями\n",
    "        lower_bound = max(lower_bound, min_value)\n",
    "        upper_bound = min(upper_bound, max_value)\n",
    "\n",
    "        # Семплирование выбросов\n",
    "        outliers = []\n",
    "        if not outliers_df.isEmpty():\n",
    "            sampled_outliers_df = outliers_df.sample(sample_fraction)\n",
    "            outliers = (\n",
    "                sampled_outliers_df\n",
    "                .select(column)\n",
    "                .limit(1000)\n",
    "                .collect()\n",
    "            )\n",
    "            outliers = [row[column] for row in outliers]\n",
    "            \n",
    "            # Добавление минимального и максимального значений, если они \n",
    "            # относятся к выбросам и не присутствуют в семпле\n",
    "            if min_value < lower_bound and min_value not in outliers:\n",
    "                outliers.append(min_value)\n",
    "            if max_value > upper_bound and max_value not in outliers:\n",
    "                outliers.append(max_value)\n",
    "\n",
    "        # Подготовка данных для axes.bxp\n",
    "        box_data.append({\n",
    "            'whislo': lower_bound,  # Нижняя граница усов\n",
    "            'q1': q1,               # Первый квартиль\n",
    "            'med': median,          # Медиана\n",
    "            'q3': q3,               # Третий квартиль\n",
    "            'whishi': upper_bound,  # Верхняя граница усов\n",
    "            'fliers': outliers      # Выбросы\n",
    "        })\n",
    "        \n",
    "    # Вывод статистических характеристик\n",
    "    print(f\"Минимальное значение:          {min_value:.2f}\")\n",
    "    print(f\"Среднее значение:              {mean_value:.2f}\")\n",
    "    print(f\"Среднеквадратичное отклонение: {std_value:.2f}\")\n",
    "    print(f\"Первый квартиль:               {q1:.2f}\")\n",
    "    print(f\"Медиана:                       {median:.2f}\")\n",
    "    print(f\"Третий квартиль:               {q3:.2f}\")\n",
    "    print(f\"Максимальное значение:         {max_value:.2f}\")\n",
    "\n",
    "    # Построение boxplot\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    ax.bxp(box_data, \n",
    "           vert=False, \n",
    "           positions=range(1, len(columns) + 1), widths=0.5)\n",
    "    ax.set_yticks(range(1, len(columns) + 1))\n",
    "    ax.set_yticklabels(columns)\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_title('Boxplots')\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(data=df, columns=[\"TOTAL_FLOOR_AREA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наблюдаем небольшое количество выбросов в данных. Для более тщательного исследования создадим функцию для визуализации распределения категориального признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quant_distribution(data: DataFrame,\n",
    "                            column: str,\n",
    "                            num_bins: int = 200) -> None:\n",
    "    \"\"\"\n",
    "    Построение гистограммы для количественной переменной с \n",
    "    использованием PySpark и Seaborn.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): DataFrame с данными.\n",
    "        column_name (str): Название колонки с количественной переменной.\n",
    "        num_bins (int): Количество бинов для гистограммы.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Находим минимальное и максимальное значения колонки\n",
    "    min_value = data.agg({column: \"min\"}).collect()[0][0]\n",
    "    max_value = data.agg({column: \"max\"}).collect()[0][0]\n",
    "\n",
    "    # Размер бина\n",
    "    bin_size = (max_value - min_value) / num_bins\n",
    "\n",
    "    # Добавляем колонку с номером бина\n",
    "    data = data.withColumn(\n",
    "        \"bin\", \n",
    "        floor((col(column) - min_value) / bin_size)\n",
    "    )\n",
    "\n",
    "    # Группируем по номеру бина и считаем количество строк в каждом бине\n",
    "    bin_counts = data.groupBy(\"bin\").count()\n",
    "\n",
    "    # Преобразуем результат в Pandas DataFrame для построения гистограммы\n",
    "    bin_counts_pd = bin_counts.limit(1000).toPandas()\n",
    "    \n",
    "    # Создаем массив границ бинов\n",
    "    bin_edges = [min_value + i * bin_size for i in range(num_bins + 2)]\n",
    "    \n",
    "    # Преобразуем номера бинов в центры бинов\n",
    "    bin_centers = [\n",
    "        (bin_edges[i] + bin_edges[i + 1]) / 2 for i in range(num_bins + 1)\n",
    "    ]\n",
    "    \n",
    "    # Добавляем центры бинов в Pandas DataFrame\n",
    "    bin_counts_pd['bin_center'] = bin_counts_pd['bin'].apply(\n",
    "        lambda x: bin_centers[int(x)]\n",
    "    )\n",
    "    \n",
    "    # Построение гистограммы с использованием Seaborn\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    sns.histplot(data=bin_counts_pd, x=\"bin_center\", \n",
    "                 weights=\"count\", kde=True, bins=num_bins + 1)\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Распределение количественного признака \\\"{column}\\\"\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quant_distribution(data=df, column=\"TOTAL_FLOOR_AREA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что почти все данные не превышают значения 300, но при этом наблюдается малое количество довольно сильных выбросов. Обрежем эти выбросы, установив для них максимальную границу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"TOTAL_FLOOR_AREA\",\n",
    "    when(col(\"TOTAL_FLOOR_AREA\") > 300.0, 300.0)\n",
    "        .otherwise(col(\"TOTAL_FLOOR_AREA\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quant_distribution(data=df, column=\"TOTAL_FLOOR_AREA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь диаграмма более эффективно представляет данные в столбце."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `NUMBER_HABITABLE_ROOMS`\n",
    "\n",
    "Признак **категориальный**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"NUMBER_HABITABLE_ROOMS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что столбцы `NUMBER_HABITABLE_ROOMS` содержит пропущенные значения. Заменим их на `0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({\"NUMBER_HABITABLE_ROOMS\": 0.0})\n",
    "count_nulls(data=df, column_name=\"NUMBER_HABITABLE_ROOMS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию расчета и визуализации распределения категориальных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_distribution(data=df, column_name=\"NUMBER_HABITABLE_ROOMS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что недвижимость в представленном датасете имеет до 10 комнат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "123\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализ столбца `NUMBER_HEATED_ROOMS`\n",
    "\n",
    "Признак **категориальный**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_nulls(data=df, column_name=\"NUMBER_HEATED_ROOMS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что столбцы `NUMBER_HEATED_ROOMS` содержит пропущенные значения. Заменим их на `0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({\"NUMBER_HEATED_ROOMS\": 0.0})\n",
    "count_nulls(data=df, column_name=\"NUMBER_HEATED_ROOMS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию расчета и визуализации распределения категориальных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cat_distribution(data=df, column_name=\"NUMBER_HEATED_ROOMS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что недвижимость в представленном датасете имеет до 10 комнат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Расчет корреляции между количественными признаками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_visualize_correlation_matrix(data: DataFrame, \n",
    "                                             columns: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Вычисляет и визуализирует корреляционную матрицу для указанных \n",
    "    колонок в DataFrame PySpark.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame PySpark.\n",
    "        columns (list[str]): Список колонок для вычисления корреляции.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Вычисление корреляционной матрицы\n",
    "    corr_matrix = {}\n",
    "    for col1 in columns:\n",
    "        corr_matrix[col1] = {}\n",
    "        for col2 in columns:\n",
    "            corr_value = data.select(corr(col1, col2)).collect()[0][0]\n",
    "            corr_matrix[col1][col2] = corr_value\n",
    "\n",
    "    # Преобразование корреляционной матрицы в DataFrame Pandas для визуализации\n",
    "    corr_matrix_pd = pd.DataFrame(corr_matrix)\n",
    "\n",
    "    # Построение и визуализация корреляционной матрицы\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix_pd, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_and_visualize_correlation_matrix(\n",
    "    data=df, columns=[\n",
    "        \"CURRENT_ENERGY_EFFICIENCY\", \"HEATING_COST_CURRENT\", \"HOT_WATER_COST_CURRENT\",  \"TOTAL_FLOOR_AREA\", \"NUMBER_HABITABLE_ROOMS\", \"NUMBER_HEATED_ROOMS\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корреляционная матрица демонстрирует наличие корреляции между некоторыми количественными признаками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, сколько объектов осталось после преобразований датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняет очищенную и обработанную таблицу на диск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение DataFrame в виде таблицы\n",
    "df.writeTo(\"sobd_lab1_processed_table\").using(\"iceberg\").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in spark.catalog.listTables():\n",
    "    print(table.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что при необходимости созданные базу данных и таблицу можно удалить следующими командами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"DROP TABLE spark_catalog.Efremenkov_database.sobd_lab1_processed_table\")\n",
    "#spark.sql(\"DROP DATABASE spark_catalog.Efremenkov_database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Останавливаем `Spark`-сессию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
